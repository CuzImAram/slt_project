{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generative LLMs for RAG\n",
    "\n",
    "This notebook provides a hands-on exploration of generative Large Language Models (LLMs). We'll start with cloud-based API models, and then explore how to run smaller models locally. Finally, we will explore different prompting strategies you can use to get best results.\n",
    "\n",
    "## Model Access & Generating an API Key\n",
    "\n",
    "For this lab, we recommend the BLABLADOR API provided by the German Supercomputing centre in JÃ¼lich. Follow [ðŸ”— their instructions](https://sdlaml.pages.jsc.fz-juelich.de/ai/guides/blablador_api_access/) to generate an API key. Use your university login to gain access. You can also interact via their [ðŸ”— Web UI](https://helmholtz-blablador.fz-juelich.de).\n",
    "\n",
    "They host several models, and you can specify the following alias names in API calls:\n",
    "- `alias-code` - Qwen2.5-Coder-7B-Instruct, a model that is specially trained for code.\n",
    "- `alias-embeddings` - GritLM-7B, a model specially made for embeddings\n",
    "- `alias-fast` - Ministral-8B-Instruct-2410, a model for high throughout (we will use this one in this lab)\n",
    "- `alias-large` - DeepSeek-R1-Distill-Llama-70B, a very large model; the most accurate, but also the slowest.\n",
    "- `alias-reasoning` - QwQ-32B, a model that is specially trained for reasoning.This model might not run 24h.\n",
    "\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Make sure to install the required libraries (comment out the following line, or make sure that your environment has these dependencies installe):\n"
   ],
   "id": "c6caf02762ddafad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:58:55.167457Z",
     "start_time": "2025-06-06T11:58:53.287286Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install openai torch transformers",
   "id": "e938b169c616d7d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (1.78.0)\n",
      "Requirement already satisfied: torch in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from torch) (80.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arama\\code\\uni\\slt\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T12:37:38.075165Z",
     "start_time": "2025-06-06T12:37:38.066132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "# load .env-Datei\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "API_URL = \"https://api.helmholtz-blablador.fz-juelich.de/v1/\"\n",
    "API_KEY = config.get(\"API_KEY\")\n",
    "API_MODEL = \"alias-fast\" # Best for fast dev runs"
   ],
   "id": "cabb82c5f6869023",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calling LLMs via an API\n",
    "\n",
    "For the first step, we are interacting with an LLM via a hosted API. Most providers (BLABLADOR too) follow an Open-AI compliant API, meaning that you can use the `openai` python wrapper also to query models by non-openai providers. First, we create an API client object:"
   ],
   "id": "a1c50168102817db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T12:37:41.649747Z",
     "start_time": "2025-06-06T12:37:40.950268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=API_URL\n",
    ")"
   ],
   "id": "abe88f8e8f80b5f7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modern LLMs are usually finetuned for instructions, and their prompting follows a turn-based pattern: each message in a conversation with the LLM has a role associated with it (`user`, the user submitting the query; `system`, general instructions at the beginning of the conversation; or `assistant`, the reply of the LLM). For our first call, we are going to use the [`completions` API endpoint](https://platform.openai.com/docs/api-reference/chat/create), which you can use in python by calling `client.chat.completions.create`.\n",
    "\n",
    "It takes 2 mandatory arguments: the model (we are going to use `alias-fast`, saved in the `API_MODEL` variable), and the message, formatted as list of `{\"role\": <role>, \"content\": <message text>}` dictionaries."
   ],
   "id": "f923bc3ee5b9c8c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T12:37:45.662994Z",
     "start_time": "2025-06-06T12:37:43.133273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make a simple completion request\n",
    "response = client.chat.completions.create(\n",
    "    model=API_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain the concept of retrieval augmented generation in 2 sentences.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "e63946a77d7fbbe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieval Augmented Generation is a method in natural language processing that combines two processes: retrieval and generation. The retrieval step involves pulling relevant information from a large dataset based on the input query, while the generation step uses this information along with a language model to produce a coherent and contextually accurate response. This approach is particularly useful for tasks like question answering, chatbots, and content creation.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Working with Temperature\n",
    "\n",
    "Of course, the API allows much more parameters to influence the results of the LLM generative process. First, `temperature`. Temperature controls \"randomness\" in the output, where `temperature = 0` yields a deterministic result, while `temperature = 1` yields a more unstable, but usually also more creative result. A balanced choice of e.g., `temperature = 0.7` is usually used.\n",
    "\n",
    "Try out how different temperature values affect the response generation for the same prompt:"
   ],
   "id": "5d6517674b293db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:30:31.547079Z",
     "start_time": "2025-05-12T08:30:29.030799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for temp in [0, 0.7, 1.0]:\n",
    "    response = client.chat.completions.create(\n",
    "        model=API_MODEL,\n",
    "        temperature=temp,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a single short advertising tagline for a retrieval augmented generation system.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(temp, response.choices[0].message.content)"
   ],
   "id": "4e7ecc9059c7c61c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  \"Unlock the Power of Retrieval-Augmented Generation: Your Ideas, Our Expertise.\"\n",
      "0.7  \"Unlock Your Words, Unleash Your Ideas.\"\n",
      "1.0  \"Got Your Thoughts? Let AI Retrieve and Generate!\"\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chat Format: System, User, and Assistant Messages\n",
    "\n",
    "Instruction-tuned LLMs generally use different message roles, which we can leverage via the API:\n",
    "\n",
    "- **system**: sets behavior instructions, is usually passed as the first message in a chat to \"set the tone\"\n",
    "- **user**: represents human input messages, the prompt as you would enter it in an LLM\n",
    "- **assistant**: represents AI responses, the generated text received by the LLM (usually from an earlier conversation turn)\n",
    "\n",
    "Try out different system prompts where you command the model to take on different personas to see how its reponse to the actual prompt differs."
   ],
   "id": "c4dcf60b3376c9e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:14.017492Z",
     "start_time": "2025-05-11T16:38:11.977572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a technical expert who explains concepts briefly in only a few sentences.\"\n",
    "user_prompt = \"What is retrieval-augmented generation?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=API_MODEL,\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "b75b34442a2c6126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieval-augmented generation (RAG) is a technique where a model retrieves relevant documents or information from an external knowledge base or index before generating a response. This helps to ensure that the generated content is accurate, up-to-date, and based on factual information. The retrieved data is then used to enhance the model's understanding and improve the quality of the output.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:34:11.545613Z",
     "start_time": "2025-05-12T08:34:09.281162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a preschool teacher explaining concepts to children. The focus is on being short and accessible.\"\n",
    "user_prompt = \"What is retrieval-augmented generation?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "67e2ea6ad88bdeb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieval-augmented generation is a type of AI that uses both a model and a database. When you ask a question, the AI first searches the database to find useful information. Then, it combines this information with its own knowledge to generate a more accurate and helpful answer. This way, it can provide more reliable information than a model that relies solely on its training data.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-turn Conversations\n",
    "\n",
    "LLMs can maintain context across multiple messages. Via the API, you can simulate conversations by appending responses and your own follow-up message to the message list. Generated responses are inserted using the `assistant` role, and the follow-up user message is then posed with the `user` role.\n",
    "\n",
    "Ask the LLM a question, append its response, and then ask a follow-up question to see how it maintains context across the whole conversation.\n"
   ],
   "id": "31d63c3414889e3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:46:17.725421Z",
     "start_time": "2025-05-12T08:46:17.722424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant providing assistance to users by guiding their learning process. You answer in short, to-the-point complete answers.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I want to learn about natural language processing and information retrieval. How should I start?\"}\n",
    "]"
   ],
   "id": "c04ec8d5b1723e42",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:38:13.835904Z",
     "start_time": "2025-05-12T08:38:05.076945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First turn\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "assistant_response = response.choices[0].message.content\n",
    "print(\"Assistant:\", assistant_response)"
   ],
   "id": "b74491492796298f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  To learn about natural language processing (NLP) and information retrieval, follow these steps:\n",
      "1. **Mathematics and Programming Basics**:\n",
      "   - Brush up on your knowledge of linear algebra, calculus, and probability.\n",
      "   - Learn Python, which is widely used in NLP and information retrieval.\n",
      "\n",
      "2. **Introduction to NLP**:\n",
      "   - Read introductory NLP books like \"Speech and Language Processing\" by Jurafsky and Martin.\n",
      "   - Take online courses on platforms like Coursera, edX, or Udacity. For example, \"Natural Language Processing with Classification and Vectors\" on Coursera.\n",
      "\n",
      "3. **Learn Key NLP Concepts**:\n",
      "   - Tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and machine translation.\n",
      "   - Study NLP libraries such as NLTK, spaCy, and Stanford NLP.\n",
      "\n",
      "4. **Information Retrieval**:\n",
      "   - Learn the basics of information retrieval, including vector space models, TF-IDF, and BM25.\n",
      "   - Study IR systems and techniques such as search engines, recommendation systems, and question answering systems.\n",
      "\n",
      "5. **Advanced Topics**:\n",
      "   - Explore advanced NLP topics like deep learning for NLP, transformers, and attention mechanisms.\n",
      "   - Study advanced IR techniques like ranking functions, clustering, and topic modeling.\n",
      "\n",
      "6. **Practical Experience**:\n",
      "   - Participate in Kaggle competitions or work on personal projects to apply what you've learned.\n",
      "   - Contribute to open-source NLP projects on GitHub.\n",
      "\n",
      "7. **Stay Updated**:\n",
      "   - Follow NLP and IR research blogs and journals.\n",
      "   - Attend workshops, conferences, and webinars.\n",
      "\n",
      "Start with the basics and gradually move to more advanced topics. Good luck with your learning!\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:25.085352Z",
     "start_time": "2025-05-11T16:38:25.081748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add the response to the conversation history; notice the 'assistant'  role\n",
    "messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "# Second turn - add your follow-up message; notice the 'user'  role\n",
    "messages.append({\"role\": \"user\", \"content\": \"What Python libraries should I use for that?\"})"
   ],
   "id": "96857cdd2811e3a7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:38:26.319160Z",
     "start_time": "2025-05-11T16:38:25.100260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "f4e1a2046e208761",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some key Python libraries for Natural Language Processing (NLP) and Information Retrieval (IR):\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running Local LLMs\n",
    "\n",
    "Besides calling models via an API, you might want to run models locally, for example if you lack internet access, for experimentation without worrying about rate limits or API cost, or when working with privacy-sensitive data. In the following, we will use models loaded from the [Huggingface]() model repository and use them with the `transformers` library. As a rule of thumb, model below 400M parameters usually fit in 16GB RAM, with acceptable inference times on CPU."
   ],
   "id": "6811982572faac7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll use a small model that can run on a CPU: [`HuggingFaceTB/SmolLM2-360M-Instruct`](). We need both the model itself, and a tokenizer to convert the message blocks into token IDs the model can consume.",
   "id": "93c0952697e0f3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:50:37.345694Z",
     "start_time": "2025-05-12T08:47:24.792540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "device = \"cpu\" # \"gpu\" for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ],
   "id": "ca88165c4ea68e8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the tokenizer to turn the message list into usable model inputs. The [`tokenizer.apply_chat_template`]() function can directly produce the desired result. Important: specify the arguments `tokenize=True` and `return_tensors=\"pt\"` to get the correct Torch tensor objects for the model. Also remember to send the tokenized data to the same device as the model using the `.to(device)` method as already with the model.",
   "id": "42e285ee55be4340"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:53:14.983163Z",
     "start_time": "2025-05-12T08:53:14.979139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the message as before\n",
    "messages = messages # Or replace with a new converstation\n",
    "# Tokenize the message\n",
    "inputs=tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\").to(device)"
   ],
   "id": "f6930572a5191b42",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now you can call the `model.generate()` function with the tokenized inputs to produce generated text. However, the model returns token IDs, not their string representation. You can convert the model output back into human-readable format using the `tokenizer.decode()` function.",
   "id": "92d40ee58b8bc867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:53:32.152553Z",
     "start_time": "2025-05-12T08:53:16.418205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the model inference pass\n",
    "outputs = model.generate(inputs, max_new_tokens=500)\n",
    "# Decode the output message\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "799977a168404cb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant providing assistance to users by guiding their learning process. You answer in short, to-the-point complete answers.<|im_end|>\n",
      "<|im_start|>user\n",
      "I want to learn about natural language processing and information retrieval. How should I start?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To start learning about natural language processing and information retrieval, begin by understanding the basics of natural language processing (NLP) and information retrieval. NLP is the field of computer science that deals with the interaction between computers and human language. It involves the development of algorithms and models to process and understand human language.\n",
      "\n",
      "Start by learning about the fundamental concepts of NLP, such as tokenization, stemming, and part-of-speech tagging. These concepts will serve as the foundation for more advanced topics. Next, explore information retrieval, which involves the process of finding relevant information from large amounts of data. This includes topics like search engines, query optimization, and information retrieval algorithms.\n",
      "\n",
      "You can start by reading books on NLP and information retrieval, or take online courses to learn the basics. Some recommended resources include Stanford University's NLP course, the University of California, Berkeley's Natural Language Processing course, and the Stanford CS231n course on Information Retrieval.<|im_end|>\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "To get the most out of any LLM, you need to carefully design prompts. Many prompting techniques exist and are hot topic in current research. The guides linked below provide a comprehensive overview on prompt engineering. In this notebook, we will explore three prompt engineering techniques:\n",
    "\n",
    "- Prompting-Induced Planning / Chain-of-Thought\n",
    "- Self-critique\n",
    "- Structured output prompting\n",
    "\n",
    "The following guides explore prompt engineering techniques in more detail:\n",
    "\n",
    "- [ðŸ”— Prompt Engineering Guide](https://drive.google.com/file/d/1AbaBYbEa_EbPelsT40-vj64L-2IwUJHy/view)\n",
    "- [ðŸ”— GPT4.1 Prompting Guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)\n"
   ],
   "id": "7a2523ef3c9a0806"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For prompt engineering, we will consider the RAG usecase with the example query and retrieved snippets below. The goal is to provide a short but informative answer to the user.",
   "id": "96da5397faf5bf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:17:40.176226Z",
     "start_time": "2025-05-12T09:17:40.172219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"why has olive oil increased in price\"\n",
    "snippets = [\n",
    "    (\"Global olive oil prices have surged due to poor harvests in Spain and Italy, caused by extreme drought and heatwaves linked to climate change.\", 0.983),\n",
    "    (\"A sharp decline in olive oil production, especially in major producing countries like Spain, has led to reduced supply and increased prices worldwide.\", 0.942),\n",
    "    (\"Increased production costs, including higher labor and transportation expenses, have contributed to the rise in olive oil prices.\", 0.891),\n",
    "    (\"Rising inflation and currency fluctuations have made imported goods, including olive oil, more expensive in several countries.\", 0.847),\n",
    "    (\"Retailers report that consumer demand for premium oils has increased, indirectly pushing prices higher across all olive oil grades.\", 0.812),\n",
    "    (\"Climate change has disrupted agricultural cycles in the Mediterranean region, impacting many crops including olives.\", 0.768),\n",
    "    (\"Sunflower oil shortages due to the war in Ukraine have led to a shift in demand toward olive oil, tightening global supply.\", 0.703),\n",
    "    (\"The Mediterranean diet, which emphasizes olive oil consumption, continues to gain popularity for its health benefits.\", 0.623),\n",
    "    (\"Spain is one of the world's largest producers of olive oil, exporting millions of liters each year.\", 0.578),\n",
    "    (\"Olive trees can live for hundreds of years and are cultivated mostly in Mediterranean climates.\", 0.519)\n",
    "]"
   ],
   "id": "457c33befff9a2f9",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vanilla RAG\n",
    "\n",
    "The most basic RAG prompting technique directly feeds retrieved context into the model alongside the query. This approach serves as a baseline: it simply instructs the model to answer the question based on the given context, without extra guidance.\n",
    "\n",
    "Implement the vanilla RAG prompt using a simple format with a system message and a user message containing the context and question."
   ],
   "id": "9cf4ddac0541163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:03:04.055077Z",
     "start_time": "2025-05-12T09:02:58.280536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a helpful assistant answering a question based on retrieved context information.\"\n",
    "user_prompt = f\"\"\"\n",
    "Context:\n",
    "{\"\\n- \".join([text for text, score in snippets])}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "a2c596b01714aff1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Olive oil prices have surged due to several factors, including:\n",
      "1. **Poor Harvests**: Extreme drought and heatwaves linked to climate change have led to poor olive oil harvests in major producing countries like Spain and Italy.\n",
      "2. **Reduced Supply**: The decrease in olive oil production has resulted in a global supply shortage, driving up prices.\n",
      "3. **Increased Production Costs**: Higher labor and transportation expenses have contributed to the rising costs of olive oil.\n",
      "4. **Inflation and Currency Fluctuations**: Rising inflation and changes in currency values have made imported goods, including olive oil, more expensive.\n",
      "5. **Shift in Demand**: A growing consumer demand for premium olive oils has indirectly pushed up prices across all grades.\n",
      "6. **Impact of Climate Change**: Climate change has disrupted agricultural cycles, affecting olive production in the Mediterranean region.\n",
      "7. **Sunflower Oil Shortages**: The war in Ukraine has led to sunflower oil shortages, shifting demand towards olive oil and further tightening global supply.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "Chain-of-thought prompting encourages the model to reason step by step before producing a final answer. This helps improve factual accuracy and clarity, especially when the retrieved context is complex or contains multiple causal links.\n",
    "\n",
    "Implement CoT prompting by asking the model to first identify relevant information, then reason through the answer, and finally summarize the conclusion."
   ],
   "id": "91862c6c096de2fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:17:37.862614Z",
     "start_time": "2025-05-12T09:17:36.165506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"You are a smart assistant answering a question using the provided context. First, identify which parts of the context are most relevant to the question. Then, briefly reason through the answer using only the relevant information. Finally, provide a short and informative answer.\"\n",
    "\n",
    "user_prompt = user_prompt # Same as before\n",
    "\n",
    "assistant_prompt = \"\"\"\n",
    "Let's think step by step:\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "ce596fd4c68c4f79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. The main reason is a sharp decline in olive oil production in major producing countries like Spain and Italy due to extreme drought and heatwaves linked to climate change.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Critique-and-Revise Prompting\n",
    "\n",
    "To improve factuality or clarity, we can instruct the model to critique an initial answer before revising it. This multi-step prompting encourages reflection and refinement, and can lead to higher-quality outputs.\n",
    "\n",
    "Generate an initial answer, prompt the model to critique it, and then ask for a revised version based on that critique."
   ],
   "id": "d9495d633540af3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:11:10.629810Z",
     "start_time": "2025-05-12T09:10:33.889474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###### Self-critique\n",
    "\n",
    "initial_response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages # Same as before\n",
    ")\n",
    "\n",
    "print(\"Initial response:\", initial_response.choices[0].message.content)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Critique this initial response, providing suggestions on how to make it better.\"})\n",
    "\n",
    "critique = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\n\\n Critique:\", critique.choices[0].message.content)\n",
    "messages.append({\"role\": \"assistant\", \"content\": critique.choices[0].message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Answer the question using initial response, but take into account the suggestions.\"})\n",
    "\n",
    "final_response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.7,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\n\\n Final response:\", final_response.choices[0].message.content)"
   ],
   "id": "9a7ef76a26c1d42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response:  Start by learning key concepts and tools in natural language processing and information retrieval. Here's a suggested pathway:\n",
      "1. **Understand the Basics:**\n",
      "   - Learn about text processing, tokenization, stemming, and lemmatization.\n",
      "   - Study basic concepts like bag-of-words, TF-IDF, and word embeddings.\n",
      "\n",
      "2. **Choose a Programming Language:**\n",
      "   - Popular choices for NLP and IR include Python, R, and Java.\n",
      "\n",
      "3. **Learn Essential Libraries and Tools:**\n",
      "   - Python: NLTK, SpaCy, Gensim, Scikit-learn, and Transformers (by Hugging Face).\n",
      "   - R: tm, text, quanteda, and text2vec.\n",
      "\n",
      "4. **Online Courses and Tutorials:**\n",
      "   - Coursera: \"Natural Language Processing Specialization\" by deeplearning.ai\n",
      "   - edX: \"Natural Language Processing with Classification and Vector Spaces\" by IIT Bombay\n",
      "   - Udacity: \"Natural Language Processing Nanodegree Foundation\"\n",
      "   - YouTube: FreeCodeCamp, 3Blue1Brown, and TensorFlow tutorials\n",
      "\n",
      "5. **Read Relevant Research Papers:**\n",
      "   - Start with introductory papers on NLP and IR, then dive deeper into specific topics like machine translation, sentiment analysis, or question answering.\n",
      "\n",
      "6. **Participate in Kaggle Competitions:**\n",
      "   - Engage in NLP and IR challenges to apply your skills and gain practical experience.\n",
      "\n",
      "7. **Join Online Communities and Forums:**\n",
      "   - Participate in discussions on platforms like Stack Overflow, Reddit (r/MachineLearning, r/learnmachinelearning), and specialized NLP and IR forums.\n",
      "\n",
      "8. **Work on Personal Projects:**\n",
      "   - Build your own NLP and IR projects to solidify your understanding and showcase your skills.\n",
      "\n",
      "9. **Stay Updated:**\n",
      "   - Follow leading research, blogs, and news in NLP and IR to stay current with the latest advancements.\n",
      "\n",
      "Start with the basics and gradually move on to more complex topics as your understanding grows. Good luck!\n",
      "\n",
      "\n",
      " Critique:  Your initial response is clear and provides a good overview of the factors contributing to the surge in olive oil prices. Here are some suggestions to make it more comprehensive and engaging:\n",
      "1. **Provide Data and Examples**: Include specific data points and examples to illustrate the extent of the problem. For instance, you could mention the percentage increase in prices or the volume of olive oil produced compared to previous years.\n",
      "2. **Explain Short-term and Long-term Impacts**: Discuss both the short-term and long-term impacts of the price surge. For example, how this affects consumers' budgets in the short term and the potential long-term consequences for the global food market.\n",
      "3. **Highlight Regional Variations**: Mention any regional variations in the impact of these factors. For instance, how the price surge affects different regions or countries differently.\n",
      "4. **Include Expert Opinions**: Incorporate quotes or summaries from experts in the field to add credibility and depth to your analysis.\n",
      "5. **Offer Potential Solutions or Mitigation Strategies**: Provide suggestions on what could be done to mitigate the effects of these factors. For example, ways to improve agricultural practices, promote sustainable production, or stabilize global supply chains.\n",
      "6. **Emphasize the Importance of Olive Oil**: Explain why olive oil is significant in the global economy and why these price fluctuations matter.\n",
      "7. **Connect to Broader Economic Trends**: Connect the olive oil price surge to broader economic trends, such as inflation, global energy prices, or geopolitical tensions.\n",
      "\n",
      "Here's an example of how you could incorporate some of these suggestions:\n",
      "\n",
      "---\n",
      "\n",
      "Olive oil prices have surged due to several interrelated factors. According to data from the International Olive Council, prices have increased by approximately 40% year-over-year. Extreme weather conditions, such as droughts and heatwaves, have led to significantly reduced harvests in major producing countries like Spain and Italy. This supply shortage has been exacerbated by higher labor and transportation costs, making olive oil production more expensive.\n",
      "\n",
      "The short-term impact of this price surge is evident in the rising costs of food items that use olive oil as an ingredient, particularly in baked goods and processed foods. Long-term effects could include a shift in consumer diets towards cheaper alternatives, potentially leading to decreased demand for olive oil in the future.\n",
      "\n",
      "Regional variations in pricing are also notable. For example, the price surge has been more pronounced in Europe due to its higher demand for olive oil. Meanwhile, in the United States, where olive oil consumption is lower, the price increase has been less significant.\n",
      "\n",
      "Experts such as Giovanni Banzana, an olive oil specialist at the Italian trade association Unapro, have warned that these factors \"create a perfect storm for higher prices.\" He suggests that investing in sustainable production methods and promoting olive oil consumption in lower-demand regions could help address the supply-demand imbalance.\n",
      "\n",
      "To mitigate the effects of these factors, stakeholders could consider supporting research into drought-resistant olive tree varieties and promoting olive oil recycling programs. Encouraging the development of alternative oil sources, such as hemp oil, could also help stabilize global supply chains.\n",
      "\n",
      "In conclusion, the olive oil price surge is a reflection of broader economic trends and environmental challenges. Addressing these issues requires a multi-faceted approach involving both immediate cost mitigation strategies and long-term sustainable solutions.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      " Final response:  Olive oil prices have surged due to several factors, including:\n",
      "1. **Poor Harvests**: Extreme drought and heatwaves linked to climate change have led to poor olive oil harvests in major producing countries like Spain and Italy.\n",
      "2. **Reduced Supply**: The decrease in olive oil production has resulted in a global supply shortage, driving up prices by approximately 40% year-over-year.\n",
      "3. **Increased Production Costs**: Higher labor and transportation expenses have contributed to the rising costs of olive oil.\n",
      "4. **Inflation and Currency Fluctuations**: Rising inflation and changes in currency values have made imported goods, including olive oil, more expensive.\n",
      "5. **Shift in Demand**: A growing consumer demand for premium olive oils has indirectly pushed up prices across all grades.\n",
      "6. **Impact of Climate Change**: Climate change has disrupted agricultural cycles, affecting olive production in the Mediterranean region.\n",
      "7. **Sunflower Oil Shortages**: The war in Ukraine has led to sunflower oil shortages, shifting demand towards olive oil and further tightening global supply.\n",
      "\n",
      "The short-term impact of this price surge is evident in the rising costs of food items that use olive oil as an ingredient, particularly in baked goods and processed foods. Long-term effects could include a shift in consumer diets towards cheaper alternatives, potentially leading to decreased demand for olive oil in the future.\n",
      "\n",
      "The price surge has been more pronounced in Europe due to its higher demand for olive oil, while in the United States, where olive oil consumption is lower, the impact has been less significant.\n",
      "\n",
      "Experts such as Giovanni Banzana, an olive oil specialist at the Italian trade association Unapro, have warned that these factors \"create a perfect storm for higher prices.\" He suggests that investing in sustainable production methods and promoting olive oil consumption in lower-demand regions could help address the supply-demand imbalance.\n",
      "\n",
      "To mitigate the effects of these factors, stakeholders could consider supporting research into drought-resistant olive tree varieties and promoting olive oil recycling programs. Encouraging the development of alternative oil sources, such as hemp oil, could also help stabilize global supply chains.\n",
      "\n",
      "In conclusion, the olive oil price surge is a reflection of broader economic trends and environmental challenges. Addressing these issues requires a multi-faceted approach involving both immediate cost mitigation strategies and long-term sustainable solutions.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Self-Consistency\n",
    "\n",
    "As we have seen before, a model might produce different outputs at repeated inference when using higher temperatures. We can use this to our advantage by prompting the model for self-consistency: first, we generate several candidate answers at high temperature (leveraging the more creative, diverse output), and then provide these to the model to distill into a final answer at low temperature (yielding consistent output).\n",
    "\n",
    "Implement self-consistency by generating 3 candidates at high temperature, and combining them in a follow-up inference pass. Design special prompts for both phases."
   ],
   "id": "9819b471157ffdf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:17:56.379443Z",
     "start_time": "2025-05-12T09:17:46.227057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_candidates = 3\n",
    "candidate_responses = []\n",
    "for i in range(num_candidates):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ]\n",
    "    response_candidate = client.chat.completions.create(\n",
    "        model=\"alias-fast\",\n",
    "        temperature=0.9,\n",
    "        messages=messages\n",
    "    )\n",
    "    candidate_responses.append(response_candidate.choices[0].message.content)\n",
    "\n",
    "print(candidate_responses)"
   ],
   "id": "1080f7da91133d44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Olive oil prices have skyrocketed due to\\n1. **Supply reduction**: Poor harvests in major producing countries like Spain and Italy, linked to extreme drought and heatwaves caused by climate change, have significantly reduced olive oil production.\\n2. **Increased demand**: The high demand for premium oils and shifting demand from sunflower oil (due to the war in Ukraine) have driven prices higher.\\n3. **Increased production costs**: Higher labor and transportation costs further elevate the price of olive oil.\\n\\nSo, the main reasons for the increased price of olive oil are reduced supply due to poor harvests and increased production costs.', ' Olive oil prices have increased due to:\\n1. **Poor Harvests**: Drought and heatwaves in Spain and Italy, caused by climate change, have significantly reduced olive oil production.\\n2. **Increased Production Costs**: Higher labor and transportation expenses have contributed to the rising costs of production.\\n3. **Global Economic Factors**: Inflation and currency fluctuations have made imported goods, including olive oil, more expensive.\\n4. **Shift in Demand**: The rising popularity of the Mediterranean diet and sunflower oil shortages due to the Ukraine war have led to increased demand for olive oil.\\n5. **Climate Change Impact**: Changes in agricultural cycles disrupted by climate change have affected olive oil production.\\n\\nSo, the prices have gone up primarily due to a combination of reduced supply due to poor harvests, higher production and transportation costs, economic factors, and increased demand.', ' 1. Reduced Production:\\n- Global olive oil prices have surged due to a sharp decline in olive oil production in major producing countries like Spain and Italy.\\n- The poor harvests in these regions are primarily caused by extreme drought and heatwaves linked to climate change.\\n\\n']\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:00.337242Z",
     "start_time": "2025-05-11T16:39:58.440779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a smart assistant answering a question based on provided candidate answers. Provide a short and informative definitive answer with only the most relevant information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\\n\\n\\n\".join([f\"Suggestion {i}: {text}\" for i, text in enumerate(candidate_responses)])},\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "62f1eb38b4ea1072",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Olive oil prices have increased due to poor harvests in major producing countries like Spain and Italy, caused by extreme drought and heatwaves linked to climate change. This reduction in supply, combined with increased production costs, currency fluctuations, and higher consumer demand for premium oils, has driven up prices worldwide.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Structured Output\n",
    "\n",
    "For downstream applications, you should have the model return structured outputs, e.g., in JSON dictionaries. Then, you can elicit responses that decompose their answer into different aspects. For example, you could refine the critique-and-revise technique from before my having the model return the critique and the final response as fields in a JSON dictionary, or separate reasoning and response in the answer in order to only display the response.\n",
    "\n",
    "For RAG, you can also use structured outputs to have the model attribute its reasoning to passages. Try to make it return its answer in a list of sentences, where for each sentence is represented as JSON dictionary with the text and the relevant passages information is taken from."
   ],
   "id": "6bbc16ee3c1ce782"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:00.365599Z",
     "start_time": "2025-05-11T16:40:00.361953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an assistant that answers questions based on retrieved context. Generate an informative but succinct answer to the question.\n",
    "Organize your answer as a list of JSON dictionaries, one for each answer passage, in the following format:\n",
    "\n",
    "[\n",
    "    {\n",
    "    \"text\": \"\", # The passage text\n",
    "    \"ref\": [1,2] # The indices of sources used in that text\n",
    "    },\n",
    "    ... # More passages\n",
    "]\n",
    "\n",
    "Answer only with valid JSON.\n",
    "\"\"\"\n",
    "context = \"\\n\".join([f\"({i}) {text}\" for i, (text, score) in enumerate(snippets)])\n",
    "user_prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n"
   ],
   "id": "dd520cf1a42cb7dc",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:07.231410Z",
     "start_time": "2025-05-11T16:40:00.401110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"alias-fast\",\n",
    "    temperature=0.1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "197c6163ecfdc912",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Olive oil prices have increased due to several factors:\n",
      "[\n",
      "    {\n",
      "    \"text\": \"Global olive oil prices have surged due to poor harvests in Spain and Italy, caused by extreme drought and heatwaves linked to climate change.\",\n",
      "    \"ref\": [0]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"A sharp decline in olive oil production, especially in major producing countries like Spain, has led to reduced supply and increased prices worldwide.\",\n",
      "    \"ref\": [1]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Increased production costs, including higher labor and transportation expenses, have contributed to the rise in olive oil prices.\",\n",
      "    \"ref\": [2]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Rising inflation and currency fluctuations have made imported goods, including olive oil, more expensive in several countries.\",\n",
      "    \"ref\": [3]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Retailers report that consumer demand for premium oils has increased, indirectly pushing prices higher across all olive oil grades.\",\n",
      "    \"ref\": [4]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Climate change has disrupted agricultural cycles in the Mediterranean region, impacting many crops including olives.\",\n",
      "    \"ref\": [5]\n",
      "    },\n",
      "    {\n",
      "    \"text\": \"Sunflower oil shortages due to the war in Ukraine have led to a shift in demand toward olive oil, tightening global supply.\",\n",
      "    \"ref\": [6]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T16:40:07.242153Z",
     "start_time": "2025-05-11T16:40:07.239346Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "250565dd8752f6a8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
